"""
ä¼æŸ¥æŸ¥çˆ¬è™«ä¸»é€»è¾‘æ¨¡å—
"""
import asyncio
import random
import re
from typing import Dict, List
from browser import BrowserManager
import config

class QccScraper:
    def __init__(self, browser_manager: BrowserManager):
        self.browser_manager = browser_manager
        self.page = browser_manager.page

    async def search_and_enter(self, company_name: str) -> tuple:
        """æœç´¢å¹¶è¿›å…¥è¯¦æƒ…é¡µï¼Œè¿”å›æ˜¯å¦æˆåŠŸå’Œå®é™…ç‚¹å‡»çš„å…¬å¸åç§°"""
        try:
            print(f"ğŸ” [1/3] æœç´¢: {company_name}")
            try:
                if "qcc.com" not in self.page.url or "search" in self.page.url:
                    await self.page.goto("https://www.qcc.com", timeout=30000)
                    await self.page.wait_for_load_state('domcontentloaded')
            except:
                await self.page.reload()

            search_input = await self.page.wait_for_selector(config.SELECTORS['search_input'], state='visible', timeout=10000)
            await search_input.fill("")
            await search_input.fill(company_name)
            await self.page.wait_for_timeout(500)
            await self.page.keyboard.press('Enter')

            await self.page.wait_for_load_state('domcontentloaded')

            target_link = self.page.locator(f"a:has-text('{company_name}')").first
            actual_click_name = company_name  # é»˜è®¤ä¸ºæœç´¢åç§°

            try:
                await target_link.wait_for(state='visible', timeout=5000)
            except:
                print(f"   âš ï¸ æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œç‚¹å‡»ç¬¬ä¸€ä¸ªç»“æœ...")
                target_link = self.page.locator("a.title").first
                # è·å–å®é™…ç‚¹å‡»çš„é“¾æ¥æ–‡æœ¬
                actual_click_name = await target_link.text_content()
                actual_click_name = actual_click_name.strip() if actual_click_name else company_name

            # è·å–å®é™…ç‚¹å‡»çš„é“¾æ¥æ–‡æœ¬
            if actual_click_name == company_name:
                actual_click_name = company_name
            else:
                actual_click_name = await target_link.text_content()
                actual_click_name = actual_click_name.strip() if actual_click_name else company_name

            async with self.page.context.expect_page() as new_page_info:
                await target_link.click()

            self.detail_page = await new_page_info.value
            await self.detail_page.wait_for_load_state('domcontentloaded')
            print(f"ğŸ“„ [2/3] è¿›å…¥è¯¦æƒ…é¡µ: {await self.detail_page.title()}")
            return True, actual_click_name
        except Exception as e:
            print(f"âŒ æœç´¢è¿›å…¥å¤±è´¥: {e}")
            return False, company_name

    async def _auto_scroll(self):
        """ã€æ–°å¢ã€‘è‡ªåŠ¨æ»šå±ï¼Œè§¦å‘æ‡’åŠ è½½"""
        print("      -> æ­£åœ¨æ»šåŠ¨é¡µé¢åŠ è½½æ•°æ®...")
        try:
            # æ»šåˆ°åº•éƒ¨
            await self.detail_page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await self.detail_page.wait_for_timeout(1000)
            # æ»šå›é¡¶éƒ¨
            await self.detail_page.evaluate("window.scrollTo(0, 0)")
            await self.detail_page.wait_for_timeout(500)
        except:
            pass

    async def _get_img_src(self, cell):
        """æå–å›¾ç‰‡é“¾æ¥"""
        try:
            img = cell.locator('img').first
            if await img.count() > 0:
                src = await img.get_attribute('src')
                if src and "default" not in src:
                    return src
                return "æœ‰å›¾ç‰‡(é»˜è®¤å›¾)"
            return ""
        except:
            return ""

    async def _resolve_real_url(self, link_element) -> str:
        """ä»ä¸­é—´è¿‡æ¸¡é¡µæå–ç›®æ ‡é“¾æ¥ï¼ˆV3.0 æºç æ‰«æç‰ˆï¼‰"""
        new_page = None
        try:
            # 1. ç‚¹å‡»é“¾æ¥ï¼Œæ•è·æ–°é¡µé¢
            async with self.page.context.expect_page() as new_page_info:
                await link_element.click()

            new_page = await new_page_info.value

            # === é˜¶æ®µ 1: ç­‰å¾…æ ¸å¿ƒå…ƒç´ å‡ºç° (å…³é”®) ===
            # ä¸è¦åªç­‰ domcontentloadedï¼Œæˆ‘ä»¬è¦ç­‰é¡µé¢ä¸ŠçœŸæ­£å‡ºç° "weibo.com" è¿™å‡ ä¸ªå­—
            # å¦‚æœ 3 ç§’å†…å‡ºç°äº†ï¼Œè¯´æ˜é¡µé¢æ¸²æŸ“å¥½äº†
            try:
                await new_page.wait_for_selector("text=weibo.com", timeout=3000)
            except:
                pass # è¶…æ—¶ä¹Ÿæ²¡å…³ç³»ï¼Œç»§ç»­å¾€ä¸‹è¯•

            # === é˜¶æ®µ 2: æ£€æŸ¥æ˜¯å¦å·²ç»è·³è½¬ ===
            if "weibo.com" in new_page.url and "qcc" not in new_page.url:
                final_url = new_page.url
                await new_page.close()
                return final_url

            # === é˜¶æ®µ 3: ä¸Šå¸è§†è§’ï¼ˆæ‰«æ HTML æºç ï¼‰ ===
            # è¿™æ˜¯æœ€ç¨³çš„ï¼Œä¸ç®¡å®ƒè—åœ¨ input é‡Œè¿˜æ˜¯ js å˜é‡é‡Œï¼Œæºç é‡Œä¸€å®šæœ‰
            print("      -> æ­£åœ¨æ‰«æé¡µé¢æºç ...")
            html_content = await new_page.content() # è·å–ç½‘é¡µæºä»£ç 

            # æ­£åˆ™åŒ¹é…ï¼šåŒ¹é… weibo.com/åé¢è·Ÿæ•°å­—æˆ–å­—æ¯çš„æ ¼å¼
            # è¿™é‡Œçš„æ­£åˆ™ç¨å¾®æ”¾å®½äº†ä¸€ç‚¹ï¼Œç¡®ä¿èƒ½åŒ¹é…åˆ°
            match = re.search(r'(https?://(?:www\.)?weibo\.com/[A-Za-z0-9_]+)', html_content)

            if match:
                real_url = match.group(1)
                # å†æ¬¡ç¡®è®¤ä¸æ˜¯ www.weibo.com (ä¸»é¡µ)ï¼Œè€Œæ˜¯å¸¦ ID çš„
                if len(real_url) > 25:
                    print(f"      -> âš¡ï¸ æºç æå–æˆåŠŸ: {real_url}")
                    await new_page.close()
                    return real_url

            # === é˜¶æ®µ 4: ç‚¹å‡»æŒ‰é’® (æœ€åçš„åŠ é€Ÿæ‰‹æ®µ) ===
            try:
                # å°è¯•ç‚¹å‡»é¡µé¢ä¸Šæ‰€æœ‰çš„ Button ç±»å‹çš„å…ƒç´ ï¼Œåªè¦åŒ…å«â€œè®¿é—®â€æˆ–â€œå‰å¾€â€
                # ä½ çš„æˆªå›¾é‡ŒæŒ‰é’®æ˜¯è“è‰²çš„ï¼Œé€šå¸¸æœ‰ btn ç±»
                btns = new_page.locator(".btn, button, a.btn").all()
                for btn in await btns:
                    txt = await btn.inner_text()
                    if "è®¿é—®" in txt or "å‰å¾€" in txt or "ç»§ç»­" in txt:
                        print(f"      -> ç‚¹å‡»æŒ‰é’®: [{txt}]")
                        await btn.click()
                        await new_page.wait_for_url("**/weibo.com/**", timeout=4000)
                        if "weibo.com" in new_page.url:
                            final_url = new_page.url
                            print(f"      -> âš¡ï¸ æŒ‰é’®è·³è½¬æˆåŠŸ: {final_url}")
                            await new_page.close()
                            return final_url
                        break
            except Exception as e:
                pass

            # === é˜¶æ®µ 5: ä¿åº•æ­»ç­‰ ===
            print("      -> åªèƒ½æ­»ç­‰è‡ªåŠ¨è·³è½¬ (6ç§’)...")
            await new_page.wait_for_timeout(6000)

            if "weibo.com" in new_page.url:
                final_url = new_page.url
                print(f"      -> ä¿åº•è·³è½¬æˆåŠŸ: {final_url}")
                await new_page.close()
                return final_url

            await new_page.close()
            return "è§£æå¤±è´¥(æœªè·³è½¬)"

        except Exception as e:
            print(f"   âš ï¸ é“¾æ¥è§£æå‡ºé”™: {e}")
            if new_page:
                try: await new_page.close()
                except: pass
            return ""

    async def extract_list_data(self, tab_name: str, col_map: dict) -> List[Dict]:
        """é€šç”¨åˆ—è¡¨æå–å‡½æ•°"""
        results = []
        try:
            # 1. ã€æš´åŠ›æŸ¥æ‰¾ Tabã€‘å°è¯•å¤šç§é€‰æ‹©å™¨
            # ä¼æŸ¥æŸ¥çš„ Tab å¯èƒ½æ˜¯ <a> ä¹Ÿå¯èƒ½æ˜¯ <li> æˆ–è€…æ˜¯ <span>
            possible_selectors = [
                f"a:has-text('{tab_name}')",
                f"li:has-text('{tab_name}')",
                f"span:has-text('{tab_name}')"
            ]

            target_tab = None
            for sel in possible_selectors:
                elements = await self.detail_page.locator(sel).all()
                for el in elements:
                    if await el.is_visible():
                        text = await el.inner_text()
                        # ç¡®ä¿ä¸æ˜¯åˆ«çš„æ— å…³é“¾æ¥ï¼Œæ¯”å¦‚åº•éƒ¨å¯¼èˆª
                        if tab_name in text and len(text) < 15:
                            target_tab = el
                            break
                if target_tab: break

            if target_tab:
                text = await target_tab.inner_text()
                # æ£€æŸ¥ (0)
                if "(0)" in text or text.strip().endswith(" 0") or text.strip() == tab_name + "0":
                    return []

                # ç‚¹å‡»å¹¶ç­‰å¾…
                await target_tab.click()
                await self.detail_page.wait_for_timeout(1000)
            else:
                # æ‰¾ä¸åˆ°Tabï¼Œå¯èƒ½å·²ç»åœ¨å½“å‰é¡µé¢æ˜¾ç¤ºäº†ï¼Œæˆ–è€…çœŸæ²¡æœ‰
                pass

            # 2. ã€æš´åŠ›æŸ¥æ‰¾è¡¨æ ¼ã€‘
            # è·å–æ‰€æœ‰è¡¨æ ¼ï¼Œé€ä¸ªæ£€æŸ¥å†…å®¹
            tables = await self.detail_page.locator('table').all()
            target_table = None

            check_text = "å¾®ä¿¡å·" if tab_name == "å¾®ä¿¡å…¬ä¼—å·" else "å¾®åšæ˜µç§°"

            for t in tables:
                table_text = await t.inner_text()
                if check_text in table_text:
                    target_table = t
                    break

            if not target_table:
                return []

            # 3. è§£æè¡¨æ ¼
            rows = await target_table.locator('tr').all()

            for row in rows:
                cols = await row.locator('td').all()
                if not cols or len(cols) < 3: continue

                item = {}

                # åºå·
                if 'seq' in col_map and len(cols) > col_map['seq']:
                    item['seq'] = await cols[col_map['seq']].inner_text()

                # å¤´åƒ
                if 'avatar' in col_map and len(cols) > col_map['avatar']:
                    item['avatar'] = await self._get_img_src(cols[col_map['avatar']])

                # åç§° & é“¾æ¥
                if 'name' in col_map and len(cols) > col_map['name']:
                    cell = cols[col_map['name']]
                    item['name'] = (await cell.inner_text()).strip()

                    link_elem = cell.locator('a').first
                    if await link_elem.count() > 0:
                        if tab_name == "å¾®åš":
                            item['link'] = await self._resolve_real_url(link_elem)
                        else:
                            item['link'] = await link_elem.get_attribute('href')
                    else:
                        item['link'] = ""

                # å¾®ä¿¡å·
                if 'wechat_id' in col_map and len(cols) > col_map['wechat_id']:
                    item['wechat_id'] = (await cols[col_map['wechat_id']].inner_text()).strip()

                # äºŒç»´ç 
                if 'qr' in col_map and len(cols) > col_map['qr']:
                    item['qr'] = await self._get_img_src(cols[col_map['qr']])

                if item:
                    results.append(item)

            return results

        except Exception as e:
            # print(f"   âš ï¸ æå– {tab_name} åˆ—è¡¨å‡ºé”™: {e}") # è°ƒè¯•æ—¶å¯æ‰“å¼€
            return []

    async def scrape_details(self, company_name: str, actual_click_name: str = None) -> Dict:
        """[3/3] æå–è¯¦ç»†ä¿¡æ¯"""
        data = {
            'company_name': company_name,
            'actual_click_name': actual_click_name or company_name,
            'wechat_list': [],
            'weibo_list': [],
            'status': 'success',
            'error': ''
        }

        try:
            # 1. ã€å…³é”®ã€‘å…ˆæ»šå±ï¼ŒåŠ è½½æ•°æ®
            await self._auto_scroll()

            # 2. å°è¯•ç‚¹å‡»çŸ¥è¯†äº§æƒå¯¼èˆª
            # å°è¯•å¤šç§å®šä½å™¨ï¼Œç›´åˆ°ç‚¹ä¸­ä¸ºæ­¢
            nav_clicked = False
            ip_selectors = [
                config.SELECTORS['nav_ip'],
                "a:has-text('çŸ¥è¯†äº§æƒ')",
                "h2:has-text('çŸ¥è¯†äº§æƒ')", # è¿™é‡Œçš„ç‚¹å‡»å¯èƒ½æ˜¯ä¸ºäº†æ»šåŠ¨çš„é”šç‚¹
                ".nav-item:has-text('çŸ¥è¯†äº§æƒ')"
            ]

            for sel in ip_selectors:
                try:
                    el = self.detail_page.locator(sel).first
                    if await el.is_visible():
                        print("   â””â”€â”€ ç‚¹å‡»ã€çŸ¥è¯†äº§æƒã€‘...")
                        await el.click()
                        await self.detail_page.wait_for_timeout(1000)
                        nav_clicked = True
                        break
                except:
                    continue

            if not nav_clicked:
                print("   âš ï¸ æœªæ‰¾åˆ°é¡¶éƒ¨å¯¼èˆªï¼Œå°è¯•ç›´æ¥æœç´¢é¡µé¢è¡¨æ ¼...")

            # 3. æŠ“å–æ•°æ®
            print("   â””â”€â”€ æŠ“å–å¾®ä¿¡å…¬ä¼—å·åˆ—è¡¨...")
            data['wechat_list'] = await self.extract_list_data("å¾®ä¿¡å…¬ä¼—å·", config.WECHAT_COL_INDEX)

            print("   â””â”€â”€ æŠ“å–å¾®åšåˆ—è¡¨...")
            data['weibo_list'] = await self.extract_list_data("å¾®åš", config.WEIBO_COL_INDEX)

        except Exception as e:
            print(f"   âŒ æå–è¯¦æƒ…å‡ºé”™: {e}")
            data['error'] = str(e)
            data['status'] = 'failed'

        finally:
            if hasattr(self, 'detail_page'):
                await self.detail_page.close()

        return data

    async def run(self, company_list: List[str]):
        """è¿è¡Œå…¥å£"""
        all_raw_data = []

        for i, company in enumerate(company_list):
            print(f"\n[{i+1}/{len(company_list)}] å¤„ç†: {company}")

            success, actual_click_name = await self.search_and_enter(company)
            if success:
                raw_data = await self.scrape_details(company, actual_click_name)
                # æ·»åŠ åŒ¹é…çŠ¶æ€åˆ°æ•°æ®ä¸­
                raw_data['match_status'] = 'åŒ¹é…' if company == actual_click_name else 'ä¸åŒ¹é…'
                all_raw_data.append(raw_data)

                wait = random.uniform(2, 4)
                print(f"   ğŸ’¤ ç­‰å¾… {wait:.1f} ç§’...")
                await self.page.wait_for_timeout(wait * 1000)
            else:
                all_raw_data.append({
                    'company_name': company,
                    'actual_click_name': company,
                    'match_status': 'search_failed',
                    'status': 'failed',
                    'error': 'æœç´¢å¤±è´¥'
                })

            if (i + 1) % 2 == 0:
                from data_handler import DataHandler
                DataHandler.save_formatted_results(all_raw_data)

        from data_handler import DataHandler
        DataHandler.save_formatted_results(all_raw_data)